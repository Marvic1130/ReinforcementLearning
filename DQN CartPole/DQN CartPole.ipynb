{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DQN (Deep Q-Network)\n",
    "\n",
    "***\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Deep Q-Network(DQN)이란?\n",
    "\n",
    "Google Deepmind에서 개발한 알고리즘으로, 강화학습에 Deep Learning을 적용하는 방식\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Q Learning\n",
    "\n",
    "강화학습은 Reward(보상)을 통해 학습하는 방식으로, 매 순간 보상이 가장 높다고 판단되는 행동을 취한다.\n",
    "\n",
    "Q Learning은 모든 가능한 상태-행동 조합을 Table로 그리고 거기에 모든 Q-Value를 적어서 사용한다. 그 후에는 벨만 방정식을 이용해서 표를 업데이트한다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"img/0.png\" width='500'>\n",
    "\n",
    "<br/>\n",
    "\n",
    "### The Bellman Equation(벨만 방정식), Q-value(Quality Value)\n",
    "\n",
    "모든 상태에서 추하는 각각의 행동에 대해 보상을 얼마나 받을지 알고있다고 가정하면 우리는 가장 높은 보상을 받을 수 있는 행동들을 연속적으로 할 수 있다. 이렇게 최종적으로 받는 모든 보상의 총합을 Q-value라고 한다.\n",
    "\n",
    "벨만 방정식이란 현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계식이다.\n",
    "\n",
    "$$\n",
    "Q(s, a) = r(s, a) + \\gamma \\max_aQ(s', a)\n",
    "$$\n",
    "\n",
    "$Q(s, a)$: 상태 $s$에서 행동 $a$를 취할 때의 $Q$ 값, 즉 행동 가치 (action-value)\n",
    "\n",
    "$r(s,a)$: 상태 $s$에서 행동 $a$를 취했을 때 얻는 즉각적인 보상 (immediate reward)\n",
    "\n",
    "$\\gamma$: 할인 계수 (discount factor)로서 미래 보상에 대한 가중치를 조절하는 요소. 이 값이 작을수록 현재 보상을 비래에 덜 중요하게 여긴다.\n",
    "\n",
    "$\\max_aQ(s', a)$: 다음 상태 $s'$에서 가능한 모든 행동 중에서 최대 $Q$ 값\n",
    "\n",
    "이 수식을 해석하면 어떠한 행동을 했을 때 받는 보상($r(s, a)$)과 다음 상태에서 가능한 모든 행동중에서 최적의 행동(미래 보상,$\\gamma \\max_aQ(s', a)$)의 합을 행동가치($Q(s, a)$)에 할당한다.\n",
    "\n",
    "### Deep Q-Network(DQN)\n",
    "\n",
    "DQN은  가치  이터레이션(Value  iteration)을  바탕으로 한 학습법이다.  알고리즘은 벨만 상태 가지 방정식(Bellman  equation)을 만족하는 Q값을 찾는 것을 목표로 하고 작동한다.\n",
    "다시말해 Deep Q-Network는 Q-Learning과 Deep Learning을 합친 방식이라고 말할 수 있다.\n",
    "DQN은 이러한 Q-가치 함수를 딥 뉴럴 네트워크로 근사화하고, 경험 재생(Experience Replay) 및 고정된 목표 네트워크(Fixed Target Network)를 사용하여 안정적인 학습을 수행한다.\n",
    "이 모델에 대한 표현은 $Q(s, a; \\theta)$이다.($\\theta$: 학습 가중치)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DQN CartPole 예제\n",
    "\n",
    "CartPole는 마찰이 없는 트랙에 막대기(pole)가 연결 되어있는 카트(cart)를 움직여 막대기를 넘어지지 않도록 하는 게임이다.\n",
    "\n",
    "에피소드가 종료되는 조건 은 아래와 같다.\n",
    "\n",
    "* 막대기가 수직으로부터 12도 이상 기울어짐 (-12도 ~ 12도).\n",
    "* 카트가 중심으로부터 2.4 이상 벗어남 (-2.4 ~ 2.4).\n",
    "* 시간 스텝이 200보다 커짐 (CartPole-v1의 경우 500).\n",
    "\n",
    "이 게임을 수행하는 프로그램을 DQN으로 학습시켜 구현하는것이 이 예제의 목표이다.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"img/1.png\" width='500'>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0. gym 설치\n",
    "\n",
    "Gym은 강화학습 알고리즘을 개발하고 비교하기 위한 툴킷이다. agent의 구조에 대해서 어떠한 가정을 하지 않으며, TensorFlow와 Theano와 같은 라이브러리와 호환 가능하다.\n",
    "\n",
    "Gym 라이브러리는 우리의 강화학습 알고리즘을 적용할 테스트 문제 (환경)들의 모음이다. 이러한 환경들은 인터페이스를 공유하며, 일반적인 알고리즘을 시도할 수 있도록 해준다.\n",
    "\n",
    "처음에 gym 0.26버전을 사용해봤는데 알 수 없는 오류가 발생하여 0.21버전을 사용했다.\n",
    "\n",
    "0.21버전을 사용했을 때 `NameError: name 'glPushMatrix' is not defined` 에러가 발생하여 s[tackoverflow.com에서 찾아본 결과](https://stackoverflow.com/questions/74314778/nameerror-name-glpushmatrix-is-not-defined) pyglet을 설치하면 해결이 된다는 글을 보고 pyglet을 설치하였다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.21 in /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow-dev/lib/python3.8/site-packages (0.21.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow-dev/lib/python3.8/site-packages (from gym==0.21) (1.23.2)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow-dev/lib/python3.8/site-packages (from gym==0.21) (2.2.0)\r\n",
      "Requirement already satisfied: pyglet==1.5.27 in /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow-dev/lib/python3.8/site-packages (1.5.27)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym==0.21\n",
    "!pip3 install pyglet==1.5.27"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 필요한 라이브러리 및 환경 로드\n",
    "\n",
    "필요한 라이브러리와 환경을 불러온다. Gym은 환경을 제공하는 라이브러리이며, TensorFlow 및 Keras는 딥 러닝 모델을 구축하는데 사용된다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. CartPole 환경 로드\n",
    "\n",
    "Gym을 사용하여 \"CartPole-v1\" 환경을 로드하고, 가능한 행동의 수를 가져온다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# CartPole 환경 로드\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "num_actions = env.action_space.n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 사용자 입력 받기\n",
    "\n",
    "사용자로부터 \"Y\" 또는 \"N\"을 입력받아서 기존에 학습된 모델을 사용할지 여부를 결정한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 사용자 입력 받기\n",
    "user_input = input(\"학습된 모델을 사용하시겠습니까? (Y/N): \")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. 모델 생성 또는 불러오기\n",
    "\n",
    "사용자 입력에 따라서 새로운 모델을 생성하거나 기존에 학습된 모델을 불러온다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 16:10:55.954180: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-09-12 16:10:55.954201: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-09-12 16:10:55.954206: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-09-12 16:10:55.954242: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-09-12 16:10:55.954256: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "if user_input.lower() == \"y\":\n",
    "    # 기존에 학습된 모델을 불러오기\n",
    "    model = tf.keras.models.load_model(\"dqn_model.h5\")\n",
    "else:\n",
    "    # Q-Network 모델 생성\n",
    "    model = Sequential([\n",
    "        Input(shape=(4,)),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dense(num_actions)\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. 모델 컴파일 및 학습 매개변수 설정"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "    # 최적화 알고리즘 및 손실 함수 설정\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    loss_fn = tf.losses.mean_squared_error\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "    # 학습 매개변수 설정\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.1\n",
    "    epsilon_decay = 0.995\n",
    "    batch_size = 32\n",
    "    gamma = 0.99"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델을 컴파일하고, 학습에 필요한 하이퍼파라미터를 설정한다.\n",
    "\n",
    "1. `optimizer = Adam(learning_rate=0.001)`:\n",
    "> 이 코드는 Adam 최적화 알고리즘을 초기화하고 학습률(learning rate)을 0.001로 설정한다.\n",
    "\n",
    "2. `loss_fn = tf.losses.mean_squared_error`:\n",
    "> 이 부분은 손실 함수(loss function)를 지정한다.\n",
    "> DQN에서는 주로 평균 제곱 오차(mean squared error)를 사용한다.\n",
    "> 모델의 예측 Q-값과 실제 Q-값 사이의 차이를 최소화하기 위해 이 손실 함수를 사용한다.\n",
    "\n",
    "3. `model.compile(optimizer=optimizer, loss=loss_fn)`:\n",
    "> 이 코드는 모델을 컴파일한다. 컴파일 단계에서는 최적화 알고리즘과 손실 함수를 모델에 연결하여 학습 프로세스를 설정한다.\n",
    "\n",
    "4. `epsilon = 1.0`:\n",
    "> 이는 탐험(exploration) 비율을 나타내며, 초기에는 높게 설정된다.\n",
    "> 탐험 비율은 에이전트가 무작위로 행동을 취할 가능성을 나타낸다.\n",
    "\n",
    "5. `epsilon_min = 0.1`:\n",
    "> 최소 탐험 비율을 나타낸다.\n",
    "> 탐험 비율은 학습이 진행됨에 따라 점차 감소하며, 이 최소값보다 낮아지지 않는다.\n",
    "\n",
    "6. `epsilon_decay = 0.995`:\n",
    "> 탐험 비율 감소율을 나타낸다.\n",
    "> 매 에피소드(또는 타임 스텝)마다 탐험 비율이 감소되어 무작위 탐험을 줄이고, 학습된 정책에 더 가까운 행동을 선택하게 된다.\n",
    "\n",
    "7. `batch_size = 32`:\n",
    "> 미니배치 크기를 나타낸다.\n",
    "> 경험 재생(Experience Replay)에서 사용되며, 학습 데이터를 작은 미니배치로 나누어 모델을 업데이트한다.\n",
    "> 작은 미니배치를 사용함으로써 학습이 안정화되고 메모리 효율성이 향상된다.\n",
    "\n",
    "8. `gamma = 0.99`:\n",
    "> 할인 계수(discount factor)로, 미래 보상을 현재 보상에 대한 가중치로 고려하는 역할을 한다.\n",
    "> 더 높은 값은 미래 보상을 더 중요하게 다룬다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Experience Replay를 위한 데이터 저장소 초기화:\n",
    "\n",
    "에피소드 데이터를 저장하기 위한 데이터 저장소를 초기화합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "    # Experience Replay를 위한 데이터 저장소 초기화\n",
    "    replay_memory = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. DQN 학습:\n",
    "주어진 에피소드 수에 따라 DQN을 학습한다.\n",
    "에피소드마다 환경을 초기화하고, epsilon-greedy 정책을 따라 행동 선택하고, 환경에서 행동을 실행하며 데이터를 수집하고, Experience Replay 및 모델 업데이트를 수행한다.\n",
    "학습이 진행됨에 따라 epsilon 값을 감소시켜 더 많은 탐험을 하도록 한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    # DQN 학습\n",
    "    num_episodes = 1000\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = model.predict(np.reshape(state, [1, 4]))\n",
    "                action = np.argmax(q_values[0])\n",
    "\n",
    "            # 환경에서 행동 실행 및 다음 상태 및 보상 얻기\n",
    "            step_result = env.step(action)\n",
    "            next_state, reward, done, info = step_result\n",
    "\n",
    "            if done:\n",
    "                reward = -10\n",
    "\n",
    "            replay_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if len(replay_memory) >= batch_size:\n",
    "                minibatch = random.sample(replay_memory, batch_size)\n",
    "                for state, action, reward, next_state, done in minibatch:\n",
    "                    target = reward\n",
    "                    if not done:\n",
    "                        target += gamma * np.amax(model.predict(np.reshape(next_state, [1, 4]))[0])\n",
    "                    target_f = model.predict(np.reshape(state, [1, 4]))\n",
    "                    target_f[0][action] = target\n",
    "                    model.fit(np.reshape(state, [1, 4]), target_f, epochs=1, verbose=0)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8. 학습된 모델 저장"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow-dev/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "    # 학습된 모델 저장\n",
    "    model.save(\"dqn_model.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9. 학습된 모델을 사용하여 CartPole 실행\n",
    "\n",
    "학습된 모델을 사용하여 CartPole을 실행하고, 에피소드의 총 보상을 확인한다.\n",
    " 실행 과정을 시각화하기 위해 env.render()를 사용한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델을 사용하여 CartPole을 실행하여 결과 확인\n",
    "for _ in range(5):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = np.argmax(model.predict(np.reshape(state, [1, 4]))[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reference\n",
    "\n",
    "> [https://engineering-ladder.tistory.com/68](https://engineering-ladder.tistory.com/68)\n",
    "> [https://jeinalog.tistory.com/20](https://jeinalog.tistory.com/20)\n",
    "> [https://myetc.tistory.com/36](https://myetc.tistory.com/36)\n",
    "> Yoonchae Kim and Dong-min Park, \"Experiment and Analysis of Learning Rate of DQN for solving Cartpole Problem,\" in 한국정보과학회 학술발표논문집, 2022, pp. 1889-1891."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
